#!/usr/bin/env python
# coding: utf-8
"""
Security Test: Demonstration of Pickle Vulnerability and Joblib Safety

This test demonstrates:
1. How pickle can be exploited to execute arbitrary code (CWE-502)
2. How joblib is safer for ML model serialization
"""

import os
import sys
import tempfile
import pickle
import joblib
from sklearn.ensemble import RandomForestRegressor
import numpy as np


class MaliciousPayload:
    """
    Malicious class that executes arbitrary code during unpickling.
    This demonstrates the pickle deserialization vulnerability.
    """
    def __reduce__(self):
        # This code will execute when the pickle is loaded
        # In a real attack, this could be: os.system('rm -rf /')
        # For safety, we just create a marker file to prove execution
        return (os.system, ('echo "EXPLOIT_EXECUTED" > /tmp/pickle_exploit_marker.txt',))


def test_pickle_vulnerability():
    """
    Test 1: Demonstrate that pickle can execute arbitrary code
    """
    print("=" * 70)
    print("TEST 1: Demonstrating Pickle Vulnerability (CWE-502)")
    print("=" * 70)
    
    # Create a temporary malicious pickle file
    temp_file = tempfile.NamedTemporaryFile(mode='wb', suffix='.pickle', delete=False)
    
    try:
        # Serialize a malicious payload
        malicious_obj = MaliciousPayload()
        pickle.dump(malicious_obj, temp_file)
        temp_file.close()
        
        print(f"✓ Created malicious pickle file: {temp_file.name}")
        
        # Remove marker file if it exists
        marker_file = '/tmp/pickle_exploit_marker.txt'
        if os.path.exists(marker_file):
            os.remove(marker_file)
        
        # Load the pickle - this will execute the malicious code
        print("⚠ Loading malicious pickle file...")
        with open(temp_file.name, 'rb') as f:
            try:
                pickle.load(f)
            except:
                pass  # May fail after execution
        
        # Check if exploit was executed
        if os.path.exists(marker_file):
            print("✗ VULNERABILITY CONFIRMED: Arbitrary code was executed!")
            print("  The pickle file executed system commands during deserialization.")
            os.remove(marker_file)
            return True
        else:
            print("✓ Code execution was blocked (system may have prevented it)")
            return False
            
    finally:
        # Cleanup
        if os.path.exists(temp_file.name):
            os.remove(temp_file.name)
    
    print()


def test_joblib_safety():
    """
    Test 2: Demonstrate that joblib is safer for legitimate ML model serialization
    """
    print("=" * 70)
    print("TEST 2: Demonstrating Joblib Safety for ML Models")
    print("=" * 70)
    
    # Create a simple ML model
    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
    y = np.array([10, 20, 30, 40])
    
    model = RandomForestRegressor(n_estimators=10, random_state=42)
    model.fit(X, y)
    
    # Save with joblib
    temp_file = tempfile.NamedTemporaryFile(mode='wb', suffix='.joblib', delete=False)
    temp_file.close()
    
    try:
        print(f"✓ Training a RandomForest model...")
        joblib.dump(model, temp_file.name)
        print(f"✓ Saved model securely with joblib: {temp_file.name}")
        
        # Load with joblib
        loaded_model = joblib.load(temp_file.name)
        print(f"✓ Loaded model safely with joblib")
        
        # Verify model works correctly
        prediction = loaded_model.predict([[9, 10]])
        print(f"✓ Model prediction works: {prediction[0]:.2f}")
        
        # Verify it's the same type
        assert type(loaded_model) == type(model), "Model type mismatch"
        print("✓ Model integrity verified")
        
        print("\n✓ SUCCESS: Joblib safely serializes/deserializes ML models")
        print("  without arbitrary code execution risks")
        return True
        
    except Exception as e:
        print(f"✗ FAILED: {str(e)}")
        return False
        
    finally:
        # Cleanup
        if os.path.exists(temp_file.name):
            os.remove(temp_file.name)
    
    print()


def test_malicious_pickle_with_joblib():
    """
    Test 3: Verify that we don't use pickle in production code
    """
    print("=" * 70)
    print("TEST 3: Verifying Production Code Uses Joblib, Not Pickle")
    print("=" * 70)
    
    # Read the main Python file
    try:
        with open('Laptop Price model(1).py', 'r') as f:
            code_content = f.read()
        
        # Check for insecure pickle usage
        has_pickle_import = 'import pickle' in code_content
        has_pickle_dump = 'pickle.dump' in code_content
        has_pickle_load = 'pickle.load' in code_content
        
        # Check for safe joblib usage
        has_joblib_import = 'import joblib' in code_content
        has_joblib_dump = 'joblib.dump' in code_content
        
        print("Code Analysis:")
        print(f"  - Uses 'import pickle': {has_pickle_import}")
        print(f"  - Uses 'pickle.dump()': {has_pickle_dump}")
        print(f"  - Uses 'pickle.load()': {has_pickle_load}")
        print(f"  - Uses 'import joblib': {has_joblib_import}")
        print(f"  - Uses 'joblib.dump()': {has_joblib_dump}")
        
        # Verify the fix
        if has_pickle_dump or has_pickle_load:
            print("\n✗ FAILED: Code still uses insecure pickle for serialization!")
            return False
        elif has_joblib_import and has_joblib_dump:
            print("\n✓ SUCCESS: Code uses secure joblib for model serialization!")
            return True
        else:
            print("\n⚠ WARNING: Could not verify serialization method")
            return False
            
    except FileNotFoundError:
        print("✗ FAILED: Could not find 'Laptop Price model(1).py'")
        return False
    
    print()


def main():
    """
    Run all security tests
    """
    print("\n" + "=" * 70)
    print("SECURITY VULNERABILITY TEST SUITE")
    print("Testing for CWE-502: Deserialization of Untrusted Data")
    print("=" * 70 + "\n")
    
    results = []
    
    # Test 1: Demonstrate pickle vulnerability
    try:
        vuln_exists = test_pickle_vulnerability()
        results.append(("Pickle Vulnerability Demo", vuln_exists))
    except Exception as e:
        print(f"Error in test 1: {e}")
        results.append(("Pickle Vulnerability Demo", False))
    
    print()
    
    # Test 2: Demonstrate joblib safety
    try:
        joblib_safe = test_joblib_safety()
        results.append(("Joblib Safety Demo", joblib_safe))
    except Exception as e:
        print(f"Error in test 2: {e}")
        results.append(("Joblib Safety Demo", False))
    
    print()
    
    # Test 3: Verify production code fix
    try:
        code_fixed = test_malicious_pickle_with_joblib()
        results.append(("Production Code Fix", code_fixed))
    except Exception as e:
        print(f"Error in test 3: {e}")
        results.append(("Production Code Fix", False))
    
    # Summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)
    
    for test_name, passed in results:
        status = "✓ PASS" if passed else "✗ FAIL"
        print(f"{status}: {test_name}")
    
    all_passed = all(passed for _, passed in results[1:])  # Skip first test (it's meant to show vulnerability)
    
    if all_passed:
        print("\n✓ All security tests passed!")
        print("  The exploit no longer works - joblib is being used securely.")
        return 0
    else:
        print("\n✗ Some tests failed - security issues may remain")
        return 1


if __name__ == "__main__":
    sys.exit(main())
