#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Security Test: Pickle Deserialization Vulnerability (CWE-502)

This test demonstrates the security vulnerability in using pickle for model
serialization and verifies that the fix using joblib with integrity checking
prevents arbitrary code execution.

VULNERABILITY: Pickle can execute arbitrary code during deserialization
FIX: Use joblib + SHA256 integrity verification
"""

import pickle
import joblib
import os
import sys
import hashlib
import json
import tempfile


class MaliciousPickle:
    """
    A class that demonstrates arbitrary code execution via pickle.
    This simulates an attacker creating a malicious pickle file.
    """
    def __reduce__(self):
        # This code will execute when unpickling
        # In a real attack, this could be: os.system('rm -rf /')
        # For safety, we just create a marker file
        return (os.system, ('echo "SECURITY_BREACH_DETECTED" > /tmp/security_test_breach.txt',))


def test_pickle_vulnerability():
    """
    Test 1: Demonstrate that pickle is vulnerable to arbitrary code execution
    """
    print("="*70)
    print("TEST 1: Demonstrating Pickle Deserialization Vulnerability (CWE-502)")
    print("="*70)
    
    # Create a malicious pickle file
    malicious_obj = MaliciousPickle()
    malicious_pickle_file = 'malicious_model.pkl'
    
    try:
        # Attacker creates malicious pickle
        with open(malicious_pickle_file, 'wb') as f:
            pickle.dump(malicious_obj, f)
        
        print("\n[ATTACKER] Created malicious pickle file")
        print(f"[ATTACKER] File: {malicious_pickle_file}")
        
        # Victim loads the pickle (this executes the malicious code)
        print("\n[VICTIM] Loading pickle file...")
        print("[!] WARNING: This will execute arbitrary code!")
        
        # Check if breach file exists before
        breach_file = '/tmp/security_test_breach.txt'
        if os.path.exists(breach_file):
            os.remove(breach_file)
        
        try:
            with open(malicious_pickle_file, 'rb') as f:
                loaded_obj = pickle.load(f)
            
            # Check if the malicious code executed
            if os.path.exists(breach_file):
                print("\n[✗] VULNERABILITY CONFIRMED: Arbitrary code executed!")
                print(f"[✗] Malicious code created file: {breach_file}")
                with open(breach_file, 'r') as f:
                    print(f"[✗] File contents: {f.read().strip()}")
                os.remove(breach_file)
                vulnerability_exists = True
            else:
                print("\n[?] Code may have executed but marker file not found")
                vulnerability_exists = True
                
        except Exception as e:
            print(f"\n[!] Exception during unpickling: {e}")
            vulnerability_exists = True
            
    finally:
        # Cleanup
        if os.path.exists(malicious_pickle_file):
            os.remove(malicious_pickle_file)
    
    print("\n" + "="*70)
    print("CONCLUSION: Pickle is UNSAFE for untrusted data")
    print("="*70)
    return vulnerability_exists


def test_joblib_with_integrity_check():
    """
    Test 2: Demonstrate that joblib + integrity verification prevents exploitation
    """
    print("\n\n" + "="*70)
    print("TEST 2: Secure Model Loading with Joblib + Integrity Verification")
    print("="*70)
    
    # Simulate a legitimate model save/load with integrity check
    print("\n[LEGITIMATE USER] Saving model securely with joblib...")
    
    # Create a dummy model (simple dict for testing)
    legitimate_model = {'model_type': 'RandomForest', 'version': '1.0'}
    model_file = 'secure_model.joblib'
    metadata_file = 'secure_model_metadata.json'
    
    try:
        # Save with joblib
        joblib.dump(legitimate_model, model_file)
        
        # Create integrity hash
        with open(model_file, 'rb') as f:
            model_bytes = f.read()
            expected_hash = hashlib.sha256(model_bytes).hexdigest()
        
        # Save metadata
        metadata = {
            'model_type': 'RandomForest',
            'sha256_hash': expected_hash,
            'version': '1.0'
        }
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"[✓] Model saved: {model_file}")
        print(f"[✓] Expected hash: {expected_hash[:32]}...")
        print(f"[✓] Metadata saved: {metadata_file}")
        
        # Test 2a: Load legitimate model with verification
        print("\n[USER] Loading model with integrity verification...")
        
        with open(metadata_file, 'r') as f:
            saved_metadata = json.load(f)
        
        with open(model_file, 'rb') as f:
            model_bytes = f.read()
            actual_hash = hashlib.sha256(model_bytes).hexdigest()
        
        if actual_hash == saved_metadata['sha256_hash']:
            print(f"[✓] Hash verification PASSED")
            print(f"[✓] Actual hash:   {actual_hash[:32]}...")
            print(f"[✓] Expected hash: {saved_metadata['sha256_hash'][:32]}...")
            loaded_model = joblib.load(model_file)
            print(f"[✓] Model loaded successfully: {loaded_model}")
            legitimate_load_success = True
        else:
            print(f"[✗] Hash verification FAILED - model may be tampered!")
            legitimate_load_success = False
        
        # Test 2b: Simulate tampering detection
        print("\n[ATTACKER] Attempting to tamper with model file...")
        
        # Tamper with the model file
        with open(model_file, 'ab') as f:
            f.write(b'\x00TAMPERED')
        
        print("[ATTACKER] Model file modified")
        
        print("\n[USER] Loading potentially tampered model...")
        
        with open(model_file, 'rb') as f:
            model_bytes = f.read()
            actual_hash = hashlib.sha256(model_bytes).hexdigest()
        
        if actual_hash == saved_metadata['sha256_hash']:
            print(f"[✗] Hash verification PASSED (should have failed!)")
            tampering_detected = False
        else:
            print(f"[✓] TAMPERING DETECTED!")
            print(f"[✓] Actual hash:   {actual_hash[:32]}...")
            print(f"[✓] Expected hash: {saved_metadata['sha256_hash'][:32]}...")
            print(f"[✓] Model loading BLOCKED - integrity check failed")
            tampering_detected = True
            
    finally:
        # Cleanup
        for f in [model_file, metadata_file]:
            if os.path.exists(f):
                os.remove(f)
    
    print("\n" + "="*70)
    print("CONCLUSION: Joblib + integrity verification provides protection")
    print("="*70)
    
    return legitimate_load_success and tampering_detected


def test_safe_loading_function():
    """
    Test 3: Demonstrate a safe model loading function
    """
    print("\n\n" + "="*70)
    print("TEST 3: Safe Model Loading Function")
    print("="*70)
    
    def safe_load_model(model_path, metadata_path):
        """
        Safely load a model with integrity verification.
        
        Args:
            model_path: Path to the joblib model file
            metadata_path: Path to the metadata JSON file
            
        Returns:
            Loaded model if verification passes, None otherwise
            
        Raises:
            ValueError: If integrity check fails
        """
        # Load metadata
        try:
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        except Exception as e:
            raise ValueError(f"Failed to load metadata: {e}")
        
        # Calculate actual hash
        try:
            with open(model_path, 'rb') as f:
                model_bytes = f.read()
                actual_hash = hashlib.sha256(model_bytes).hexdigest()
        except Exception as e:
            raise ValueError(f"Failed to read model file: {e}")
        
        # Verify integrity
        expected_hash = metadata.get('sha256_hash')
        if not expected_hash:
            raise ValueError("Metadata missing SHA256 hash")
        
        if actual_hash != expected_hash:
            raise ValueError(
                f"Model integrity check FAILED!\n"
                f"Expected: {expected_hash[:32]}...\n"
                f"Actual:   {actual_hash[:32]}...\n"
                f"The model file may have been tampered with."
            )
        
        # Load model with joblib
        try:
            model = joblib.load(model_path)
            print(f"[✓] Model loaded successfully with integrity verification")
            return model
        except Exception as e:
            raise ValueError(f"Failed to load model: {e}")
    
    # Test the function
    model_file = 'test_model.joblib'
    metadata_file = 'test_model_metadata.json'
    
    try:
        # Create test model
        test_model = {'type': 'test', 'data': [1, 2, 3]}
        joblib.dump(test_model, model_file)
        
        with open(model_file, 'rb') as f:
            model_hash = hashlib.sha256(f.read()).hexdigest()
        
        metadata = {'sha256_hash': model_hash, 'model_type': 'test'}
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f)
        
        print("\n[TEST] Testing safe_load_model() function...")
        
        # Test 1: Valid model
        print("\n[TEST] Loading valid model...")
        model = safe_load_model(model_file, metadata_file)
        print(f"[✓] Successfully loaded: {model}")
        test_passed = True
        
        # Test 2: Tampered model
        print("\n[TEST] Loading tampered model...")
        with open(model_file, 'ab') as f:
            f.write(b'TAMPERED')
        
        try:
            model = safe_load_model(model_file, metadata_file)
            print(f"[✗] ERROR: Tampered model was loaded (should have failed)")
            test_passed = False
        except ValueError as e:
            print(f"[✓] Tampering correctly detected and blocked")
            print(f"[✓] Error message: {str(e)[:80]}...")
            
    finally:
        # Cleanup
        for f in [model_file, metadata_file]:
            if os.path.exists(f):
                os.remove(f)
    
    print("\n" + "="*70)
    print("CONCLUSION: Safe loading function prevents loading tampered models")
    print("="*70)
    
    return test_passed


def run_all_tests():
    """
    Run all security tests
    """
    print("\n" + "#"*70)
    print("# SECURITY TEST SUITE: Pickle Deserialization Vulnerability")
    print("# CWE-502: Deserialization of Untrusted Data")
    print("#"*70)
    
    print("\n[INFO] This test suite demonstrates:")
    print("  1. The vulnerability in using pickle for model serialization")
    print("  2. How joblib + integrity verification prevents exploitation")
    print("  3. A safe model loading function implementation")
    
    results = {}
    
    # Run tests
    results['pickle_vulnerable'] = test_pickle_vulnerability()
    results['joblib_secure'] = test_joblib_with_integrity_check()
    results['safe_function'] = test_safe_loading_function()
    
    # Summary
    print("\n\n" + "#"*70)
    print("# TEST SUMMARY")
    print("#"*70)
    
    print(f"\n{'Test':<50} {'Result':<10}")
    print("-"*70)
    print(f"{'1. Pickle vulnerability demonstrated':<50} {'✓ PASS' if results['pickle_vulnerable'] else '✗ FAIL'}")
    print(f"{'2. Joblib + integrity verification secure':<50} {'✓ PASS' if results['joblib_secure'] else '✗ FAIL'}")
    print(f"{'3. Safe loading function works':<50} {'✓ PASS' if results['safe_function'] else '✗ FAIL'}")
    
    all_passed = all(results.values())
    
    print("\n" + "="*70)
    if all_passed:
        print("✓ ALL TESTS PASSED - Vulnerability fixed and verified")
    else:
        print("✗ SOME TESTS FAILED - Review implementation")
    print("="*70)
    
    print("\n[SECURITY RECOMMENDATIONS]")
    print("1. Always use joblib instead of pickle for sklearn models")
    print("2. Implement integrity verification using SHA256 hashes")
    print("3. Store model metadata separately from model files")
    print("4. Validate model files before loading in production")
    print("5. Never load models from untrusted sources without verification")
    print("6. Consider signing models with cryptographic signatures")
    print("7. Implement model versioning and audit logs")
    
    return all_passed


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
